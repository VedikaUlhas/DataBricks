{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fca8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/User/LearnlyticaAssessments/DTAB/Q225/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "text_file = spark.sparkContext.textFile(\"sample.txt\")\n",
    "\n",
    "counts = (\n",
    "    text_file.flatMap(lambda line: line.split(\" \"))\n",
    "    .map(lambda word: (word, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "print(counts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55848cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "empdf = spark.read.csv(\"C:\\datasets\\datasets\\emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d0cccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|_c0|_c1| _c2|\n",
      "+---+---+----+\n",
      "|111|zzz|8000|\n",
      "|111|aaa|8888|\n",
      "|121|bbb|8000|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff6624f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= spark.sparkContext\n",
    "emprdd = sc.textFile(\"C:\\datasets\\datasets\\emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "560edd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111,zzz,8000', '111,aaa,8888', '121,bbb,8000']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emprdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c719eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "text_file = spark.sparkContext.textFile(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a80b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsrdd = text_file.flatMap(lambda line: line.split(\" \") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6991cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = wordsrdd.map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e72982d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcountrdd = mapped_rdd.reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131daf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://10.33.198.42:4040'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62303ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample.txt MapPartitionsRDD[44] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d019700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8eba4b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these are words',\n",
       " 'these are more words',\n",
       " 'words in english',\n",
       " 'spark spark spark demo file for word count ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "942d2c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['these',\n",
       " 'are',\n",
       " 'words',\n",
       " 'these',\n",
       " 'are',\n",
       " 'more',\n",
       " 'words',\n",
       " 'words',\n",
       " 'in',\n",
       " 'english',\n",
       " 'spark',\n",
       " 'spark',\n",
       " 'spark',\n",
       " 'demo',\n",
       " 'file',\n",
       " 'for',\n",
       " 'word',\n",
       " 'count',\n",
       " '']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a4894fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('these', 1),\n",
       " ('are', 1),\n",
       " ('words', 1),\n",
       " ('these', 1),\n",
       " ('are', 1),\n",
       " ('more', 1),\n",
       " ('words', 1),\n",
       " ('words', 1),\n",
       " ('in', 1),\n",
       " ('english', 1),\n",
       " ('spark', 1),\n",
       " ('spark', 1),\n",
       " ('spark', 1),\n",
       " ('demo', 1),\n",
       " ('file', 1),\n",
       " ('for', 1),\n",
       " ('word', 1),\n",
       " ('count', 1),\n",
       " ('', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dab21918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('these', 2),\n",
       " ('are', 2),\n",
       " ('words', 3),\n",
       " ('more', 1),\n",
       " ('demo', 1),\n",
       " ('file', 1),\n",
       " ('for', 1),\n",
       " ('word', 1),\n",
       " ('', 1),\n",
       " ('in', 1),\n",
       " ('english', 1),\n",
       " ('spark', 3),\n",
       " ('count', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcountrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52e82f51",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o204.saveAsTextFile.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwordcountrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwordcount_out\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\pyspark\\rdd.py:3425\u001b[39m, in \u001b[36mRDD.saveAsTextFile\u001b[39m\u001b[34m(self, path, compressionCodecClass)\u001b[39m\n\u001b[32m   3423\u001b[39m     keyed._jrdd.map(\u001b[38;5;28mself\u001b[39m.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)\n\u001b[32m   3424\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3425\u001b[39m     \u001b[43mkeyed\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o204.saveAsTextFile.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "wordcountrdd.saveAsTextFile(\"wordcount_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58073aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"C:\\datasets\\datasets\\patient.csv\"\n",
    "\n",
    "patient_df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ba83dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+------+------------+\n",
      "|patient_id|pname|   drug|gender|sales_amount|\n",
      "+----------+-----+-------+------+------------+\n",
      "|       111|  aaa|   Para|     m|         900|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb| Crocin|     f|         600|\n",
      "|       222|  bbb|   Para|     f|         999|\n",
      "|       333|  ccc| calpol|     f|         500|\n",
      "|       444|  ddd|    hcq|     m|         500|\n",
      "|       111|  aaa|    hcq|     m|         600|\n",
      "|       555|  eee|cetzine|     m|         700|\n",
      "|       333|  ccc|   Para|     m|         444|\n",
      "|       111|  aaa|   Para|     m|         500|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb| Crocin|     f|         600|\n",
      "|       222|  bbb|   Para|     f|         999|\n",
      "|       333|  ccc| calpol|     f|         500|\n",
      "|       444|  ddd|    hcq|     m|         500|\n",
      "|       111|  aaa|    hcq|     m|         600|\n",
      "|       555|  eee|cetzine|     m|         700|\n",
      "|       333|  ccc|   Para|     m|         444|\n",
      "|       111|  aaa|   Para|     m|         500|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "+----------+-----+-------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "856c19ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patient_id', 'int'),\n",
       " ('pname', 'string'),\n",
       " ('drug', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('sales_amount', 'int')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aef6d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- pname: string (nullable = true)\n",
      " |-- drug: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- sales_amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35d96e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+------+------------+\n",
      "|patient_id|pname|   drug|gender|sales_amount|\n",
      "+----------+-----+-------+------+------------+\n",
      "|       111|  aaa|   Para|     m|         900|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb| Crocin|     f|         600|\n",
      "|       222|  bbb|   Para|     f|         999|\n",
      "|       333|  ccc| calpol|     f|         500|\n",
      "+----------+-----+-------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1aada207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(patient_id=111, pname='aaa', drug='Para', gender='m', sales_amount=900),\n",
       " Row(patient_id=111, pname='aaa', drug='metacin', gender='m', sales_amount=800),\n",
       " Row(patient_id=222, pname='bbb', drug='Crocin', gender='f', sales_amount=600)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8108fdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(patient_id=333, pname='ccc', drug='Para', gender='m', sales_amount=444)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "471a7792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+------+------------+\n",
      "|patient_id|pname|   drug|gender|sales_amount|\n",
      "+----------+-----+-------+------+------------+\n",
      "|       111|  aaa|   Para|     m|         900|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb| Crocin|     f|         600|\n",
      "+----------+-----+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0154b4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(patient_id=111, pname='aaa', drug='Para', gender='m', sales_amount=900)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "124cc92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+------+------------+\n",
      "|patient_id|pname|drug|gender|sales_amount|\n",
      "+----------+-----+----+------+------------+\n",
      "|       111|  aaa|Para|     m|         900|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       333|  ccc|Para|     m|         444|\n",
      "|       111|  aaa|Para|     m|         500|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       333|  ccc|Para|     m|         444|\n",
      "|       111|  aaa|Para|     m|         500|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       333|  ccc|Para|     m|         444|\n",
      "+----------+-----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.filter(\"Drug = 'Para'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04df1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+------+------------+\n",
      "|patient_id|pname|drug|gender|sales_amount|\n",
      "+----------+-----+----+------+------------+\n",
      "|       111|  aaa|Para|     m|         900|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       333|  ccc|Para|     m|         444|\n",
      "|       111|  aaa|Para|     m|         500|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       333|  ccc|Para|     m|         444|\n",
      "|       111|  aaa|Para|     m|         500|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       333|  ccc|Para|     m|         444|\n",
      "+----------+-----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.where(\"Drug='Para'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d29282c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+------+------------+\n",
      "|patient_id|pname|drug|gender|sales_amount|\n",
      "+----------+-----+----+------+------------+\n",
      "|       111|  aaa|Para|     m|         900|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "|       222|  bbb|Para|     f|         999|\n",
      "+----------+-----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.where(\"sales_amount>800\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a0b0199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+------+------------+\n",
      "|patient_id|pname|drug|gender|sales_amount|\n",
      "+----------+-----+----+------+------------+\n",
      "|       111|  aaa|Para|     m|         900|\n",
      "+----------+-----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.where(\"sales_amount>800 and gender ='m'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1214287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+------+------------+\n",
      "|patient_id|pname|   drug|gender|sales_amount|\n",
      "+----------+-----+-------+------+------------+\n",
      "|       111|  aaa|   Para|     m|         900|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb|   Para|     f|         999|\n",
      "|       444|  ddd|    hcq|     m|         500|\n",
      "|       111|  aaa|    hcq|     m|         600|\n",
      "|       555|  eee|cetzine|     m|         700|\n",
      "|       333|  ccc|   Para|     m|         444|\n",
      "|       111|  aaa|   Para|     m|         500|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb|   Para|     f|         999|\n",
      "|       444|  ddd|    hcq|     m|         500|\n",
      "|       111|  aaa|    hcq|     m|         600|\n",
      "|       555|  eee|cetzine|     m|         700|\n",
      "|       333|  ccc|   Para|     m|         444|\n",
      "|       111|  aaa|   Para|     m|         500|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "|       222|  bbb|   Para|     f|         999|\n",
      "|       444|  ddd|    hcq|     m|         500|\n",
      "|       111|  aaa|    hcq|     m|         600|\n",
      "|       555|  eee|cetzine|     m|         700|\n",
      "+----------+-----+-------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.where(\"sales_amount>800 or gender ='m'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edb84a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "950aaff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2f7dac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   drug|\n",
      "+-------+\n",
      "| calpol|\n",
      "|    hcq|\n",
      "|cetzine|\n",
      "|metacin|\n",
      "| Crocin|\n",
      "|   Para|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.select(\"drug\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0838319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+------+------------+\n",
      "|patient_id|pname|   drug|gender|sales_amount|\n",
      "+----------+-----+-------+------+------------+\n",
      "|       222|  bbb| Crocin|     f|         600|\n",
      "|       111|  aaa|   Para|     m|         900|\n",
      "|       333|  ccc| calpol|     f|         500|\n",
      "|       555|  eee|cetzine|     m|         700|\n",
      "|       444|  ddd|    hcq|     m|         500|\n",
      "|       111|  aaa|metacin|     m|         800|\n",
      "+----------+-----+-------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_df.dropDuplicates([\"drug\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56932803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = patient_df.dropDuplicates([\"drug\"])\n",
    "p1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "40d53288",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = patient_df.select(\"pname\",\"sales_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a783fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|pname|sales_amount|\n",
      "+-----+------------+\n",
      "|  aaa|         900|\n",
      "|  aaa|         800|\n",
      "|  bbb|         600|\n",
      "|  bbb|         999|\n",
      "|  ccc|         500|\n",
      "|  ddd|         500|\n",
      "|  aaa|         600|\n",
      "|  eee|         700|\n",
      "|  ccc|         444|\n",
      "|  aaa|         500|\n",
      "|  aaa|         800|\n",
      "|  bbb|         600|\n",
      "|  bbb|         999|\n",
      "|  ccc|         500|\n",
      "|  ddd|         500|\n",
      "|  aaa|         600|\n",
      "|  eee|         700|\n",
      "|  ccc|         444|\n",
      "|  aaa|         500|\n",
      "|  aaa|         800|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4f1630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_path = \"C:\\datasets\\datasets\\superstore.csv\"\n",
    "sales_df = spark.read.csv(sales_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d0565c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+----------+------------+----------+-------------+---------+-------------+---------------+-------------+----------+------+-------+---------------+----------+------------+--------------------+-------+--------+--------+--------+------------+-------------+\n",
      "|   ID|       OrderID| OrderDate|  ShipDate|    ShipMode|CustomerID| CustomerName|  Segment|         City|          State|      Country|PostalCode|Market| Region|      ProductID|  Category|Sub-Category|         ProductName|  Sales|Quantity|Discount|  Profit|ShippingCost|OrderPriority|\n",
      "+-----+--------------+----------+----------+------------+----------+-------------+---------+-------------+---------------+-------------+----------+------+-------+---------------+----------+------------+--------------------+-------+--------+--------+--------+------------+-------------+\n",
      "|32298|CA-2012-124891|31/07/2012|31/07/2012|    Same Day|  RH-19495|  Rick Hansen| Consumer|New York City|       New York|United States|     10024|    US|   East|TEC-AC-10003033|Technology| Accessories|Plantronics CS510...|2309.65|       7|       0|762.1845|     933.57 |     Critical|\n",
      "|26341| IN-2013-77878|05/02/2013|07/02/2013|Second Class|  JR-16210|Justin Ritter|Corporate|   Wollongong|New South Wales|    Australia|      NULL|  APAC|Oceania|FUR-CH-10003950| Furniture|      Chairs|Novimex Executive...|  Black|3709.395|       9|     0.1|    -288.765|      923.63 |\n",
      "+-----+--------------+----------+----------+------------+----------+-------------+---------+-------------+---------------+-------------+----------+------+-------+---------------+----------+------------+--------------------+-------+--------+--------+--------+------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89142307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f9cdd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+\n",
      "|country|         state|              city|\n",
      "+-------+--------------+------------------+\n",
      "|  India|       Gujarat|             Surat|\n",
      "|  India|       Haryana|         Gorakhpur|\n",
      "|  India|        Kerala|Thiruvananthapuram|\n",
      "|  India|     Jharkhand|        Jamshedpur|\n",
      "|  India|Madhya Pradesh|            Bhopal|\n",
      "|  India|         Delhi|             Delhi|\n",
      "|  India|   Uttarakhand|            Raipur|\n",
      "|  India|   West Bengal|           Naihati|\n",
      "|  India|         Delhi|             Delhi|\n",
      "|  India|       Gujarat|          Vadodara|\n",
      "|  India|         Bihar|        Aurangabad|\n",
      "|  India|       Haryana|         Gorakhpur|\n",
      "|  India|     Jharkhand|            Bokaro|\n",
      "|  India|        Kerala|         Kozhikode|\n",
      "|  India|Madhya Pradesh|          Jabalpur|\n",
      "|  India|         Bihar|             Patna|\n",
      "|  India|        Kerala|             Kochi|\n",
      "|  India| Uttar Pradesh|              Agra|\n",
      "|  India|   Uttarakhand|         Dehra Dun|\n",
      "|  India|  Chhattisgarh|              Kota|\n",
      "+-------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='India'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32669ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1555"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='India'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a2f22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='India'\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e1c21e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+\n",
      "|country|         state|         city|\n",
      "+-------+--------------+-------------+\n",
      "|  India|Andhra Pradesh|    Anantapur|\n",
      "|  India|Andhra Pradesh|       Guntur|\n",
      "|  India|Andhra Pradesh|     Kakinada|\n",
      "|  India|Andhra Pradesh|      Nellore|\n",
      "|  India|Andhra Pradesh|     Tirupati|\n",
      "|  India|Andhra Pradesh|   Vijayawada|\n",
      "|  India|Andhra Pradesh|Visakhapatnam|\n",
      "|  India|         Assam|     Guwahati|\n",
      "|  India|         Bihar|   Aurangabad|\n",
      "|  India|         Bihar| Bihar Sharif|\n",
      "|  India|         Bihar|    Darbhanga|\n",
      "|  India|         Bihar|         Gaya|\n",
      "|  India|         Bihar|  Muzaffarpur|\n",
      "|  India|         Bihar|        Patna|\n",
      "|  India|    Chandigarh|   Chandigarh|\n",
      "|  India|  Chhattisgarh|       Bhilai|\n",
      "|  India|  Chhattisgarh|         Durg|\n",
      "|  India|  Chhattisgarh|        Korba|\n",
      "|  India|  Chhattisgarh|         Kota|\n",
      "|  India|  Chhattisgarh|       Raipur|\n",
      "+-------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='India'\").dropDuplicates().sort(\"state\",\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03bc31a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+\n",
      "|country|         state|         city|\n",
      "+-------+--------------+-------------+\n",
      "|  India|Andhra Pradesh|    Anantapur|\n",
      "|  India|Andhra Pradesh|       Guntur|\n",
      "|  India|Andhra Pradesh|     Kakinada|\n",
      "|  India|Andhra Pradesh|      Nellore|\n",
      "|  India|Andhra Pradesh|     Tirupati|\n",
      "|  India|Andhra Pradesh|   Vijayawada|\n",
      "|  India|Andhra Pradesh|Visakhapatnam|\n",
      "|  India|         Assam|     Guwahati|\n",
      "|  India|         Bihar|   Aurangabad|\n",
      "|  India|         Bihar| Bihar Sharif|\n",
      "|  India|         Bihar|    Darbhanga|\n",
      "|  India|         Bihar|         Gaya|\n",
      "|  India|         Bihar|  Muzaffarpur|\n",
      "|  India|         Bihar|        Patna|\n",
      "|  India|    Chandigarh|   Chandigarh|\n",
      "|  India|  Chhattisgarh|       Bhilai|\n",
      "|  India|  Chhattisgarh|         Durg|\n",
      "|  India|  Chhattisgarh|        Korba|\n",
      "|  India|  Chhattisgarh|         Kota|\n",
      "|  India|  Chhattisgarh|       Raipur|\n",
      "+-------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='India'\").dropDuplicates().orderBy(\"state\",\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f54a6f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+\n",
      "|country|         state|         city|\n",
      "+-------+--------------+-------------+\n",
      "|  India|Andhra Pradesh|Visakhapatnam|\n",
      "|  India|Andhra Pradesh|   Vijayawada|\n",
      "|  India|Andhra Pradesh|     Tirupati|\n",
      "|  India|Andhra Pradesh|      Nellore|\n",
      "|  India|Andhra Pradesh|     Kakinada|\n",
      "|  India|Andhra Pradesh|       Guntur|\n",
      "|  India|Andhra Pradesh|    Anantapur|\n",
      "|  India|         Assam|     Guwahati|\n",
      "|  India|         Bihar|        Patna|\n",
      "|  India|         Bihar|  Muzaffarpur|\n",
      "|  India|         Bihar|         Gaya|\n",
      "|  India|         Bihar|    Darbhanga|\n",
      "|  India|         Bihar| Bihar Sharif|\n",
      "|  India|         Bihar|   Aurangabad|\n",
      "|  India|    Chandigarh|   Chandigarh|\n",
      "|  India|  Chhattisgarh|       Raipur|\n",
      "|  India|  Chhattisgarh|         Kota|\n",
      "|  India|  Chhattisgarh|        Korba|\n",
      "|  India|  Chhattisgarh|         Durg|\n",
      "|  India|  Chhattisgarh|       Bhilai|\n",
      "+-------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\")\\\n",
    "    .filter(\"country='India'\")\\\n",
    "    .dropDuplicates().\\\n",
    "    orderBy([\"state\",\"city\"],ascending=[1,0])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e29ebafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+\n",
      "|country|         state|         city|\n",
      "+-------+--------------+-------------+\n",
      "|  India|Andhra Pradesh|    Anantapur|\n",
      "|  India|Andhra Pradesh|       Guntur|\n",
      "|  India|Andhra Pradesh|     Kakinada|\n",
      "|  India|Andhra Pradesh|      Nellore|\n",
      "|  India|Andhra Pradesh|     Tirupati|\n",
      "|  India|Andhra Pradesh|   Vijayawada|\n",
      "|  India|Andhra Pradesh|Visakhapatnam|\n",
      "|  India|         Assam|     Guwahati|\n",
      "|  India|         Bihar|   Aurangabad|\n",
      "|  India|         Bihar| Bihar Sharif|\n",
      "|  India|         Bihar|    Darbhanga|\n",
      "|  India|         Bihar|         Gaya|\n",
      "|  India|         Bihar|  Muzaffarpur|\n",
      "|  India|         Bihar|        Patna|\n",
      "|  India|    Chandigarh|   Chandigarh|\n",
      "|  India|  Chhattisgarh|       Bhilai|\n",
      "|  India|  Chhattisgarh|         Durg|\n",
      "|  India|  Chhattisgarh|        Korba|\n",
      "|  India|  Chhattisgarh|         Kota|\n",
      "|  India|  Chhattisgarh|       Raipur|\n",
      "+-------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\")\\\n",
    "    .filter(\"country='India'\")\\\n",
    "    .dropDuplicates().\\\n",
    "    orderBy([\"state\",\"city\"],ascending=[1,1])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f954323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------+\n",
      "|country|        state|        city|\n",
      "+-------+-------------+------------+\n",
      "|  India|  West Bengal|    Panihati|\n",
      "|  India|  West Bengal|     Naihati|\n",
      "|  India|  West Bengal|       Kulti|\n",
      "|  India|  West Bengal|   Kamarhati|\n",
      "|  India|  West Bengal|    Durgapur|\n",
      "|  India|  West Bengal|    Bhatpara|\n",
      "|  India|  West Bengal|  Barddhaman|\n",
      "|  India|  West Bengal|     Barasat|\n",
      "|  India|  West Bengal|   Baranagar|\n",
      "|  India|  West Bengal|     Asansol|\n",
      "|  India|  Uttarakhand|    Srinagar|\n",
      "|  India|  Uttarakhand|      Raipur|\n",
      "|  India|  Uttarakhand|   Dehra Dun|\n",
      "|  India|Uttar Pradesh|    Varanasi|\n",
      "|  India|Uttar Pradesh|Shahjahanpur|\n",
      "|  India|Uttar Pradesh|  Saharanpur|\n",
      "|  India|Uttar Pradesh|   Moradabad|\n",
      "|  India|Uttar Pradesh|    Mirzapur|\n",
      "|  India|Uttar Pradesh|      Meerut|\n",
      "|  India|Uttar Pradesh|         Mau|\n",
      "+-------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\")\\\n",
    "    .filter(\"country='India'\")\\\n",
    "    .dropDuplicates().\\\n",
    "    orderBy([\"state\",\"city\"],ascending=[0,0])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "79f95969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------+\n",
      "|country|        state|      city|\n",
      "+-------+-------------+----------+\n",
      "|  India|  West Bengal|   Asansol|\n",
      "|  India|  West Bengal| Baranagar|\n",
      "|  India|  West Bengal|   Barasat|\n",
      "|  India|  West Bengal|Barddhaman|\n",
      "|  India|  West Bengal|  Bhatpara|\n",
      "|  India|  West Bengal|  Durgapur|\n",
      "|  India|  West Bengal| Kamarhati|\n",
      "|  India|  West Bengal|     Kulti|\n",
      "|  India|  West Bengal|   Naihati|\n",
      "|  India|  West Bengal|  Panihati|\n",
      "|  India|  Uttarakhand| Dehra Dun|\n",
      "|  India|  Uttarakhand|    Raipur|\n",
      "|  India|  Uttarakhand|  Srinagar|\n",
      "|  India|Uttar Pradesh|      Agra|\n",
      "|  India|Uttar Pradesh|   Aligarh|\n",
      "|  India|Uttar Pradesh| Allahabad|\n",
      "|  India|Uttar Pradesh|  Bareilly|\n",
      "|  India|Uttar Pradesh|  Bilaspur|\n",
      "|  India|Uttar Pradesh|    Etawah|\n",
      "|  India|Uttar Pradesh| Firozabad|\n",
      "+-------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\")\\\n",
    "    .filter(\"country='India'\")\\\n",
    "    .dropDuplicates().\\\n",
    "    orderBy([\"state\",\"city\"],ascending=[0,1])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "41d16e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_path = \"C:\\datasets\\datasets\\Retail\\customer.csv\"\n",
    "customer_df = spark.read.csv(customer_path,sep=\"\\t\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c15e6fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "|C_CUSTKEY|            C_NAME|           C_ADDRESS|C_NATIONKEY|        C_PHONE|C_ACCTBAL|C_MKTSEGMENT|           C_COMMENT|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "|        1|Customer#000000001|   IVhzIApeRb ot,c,E|         15|25-989-741-2988|   711.56|    BUILDING|to the even, regu...|\n",
      "|        2|Customer#000000002|XSTf4,NCwDVaWNe6t...|         13|23-768-687-3665|   121.65|  AUTOMOBILE|l accounts. blith...|\n",
      "|        3|Customer#000000003|        MG9kdTD2WBHm|          1|11-719-748-3364|  7498.12|  AUTOMOBILE| deposits eat sly...|\n",
      "|        4|Customer#000000004|         XxVSJsLAGtn|          4|14-128-190-5944|  2866.83|   MACHINERY| requests. final,...|\n",
      "|        5|Customer#000000005|KvpyuHCplrB84WgAi...|          3|13-750-942-6364|   794.47|   HOUSEHOLD|n accounts will h...|\n",
      "|        6|Customer#000000006|sKZz0CsnMD7mp4Xd0...|         20|30-114-968-4951|  7638.57|  AUTOMOBILE|tions. even depos...|\n",
      "|        7|Customer#000000007|TcGe5gaZNgVePxU5k...|         18|28-190-982-9759|  9561.95|  AUTOMOBILE|ainst the ironic,...|\n",
      "|        8|Customer#000000008|I0B10bB0AymmC, 0P...|         17|27-147-574-9335|  6819.74|    BUILDING|among the slyly r...|\n",
      "|        9|Customer#000000009|xKiAFTjUsCuxfeleN...|          8|18-338-906-3675|  8324.07|   FURNITURE|r theodolites acc...|\n",
      "|       10|Customer#000000010|6LrEaV6KR6PLVcgl2...|          5|15-741-346-9870|  2753.54|   HOUSEHOLD|es regular deposi...|\n",
      "|       11|Customer#000000011|PkWS 3HlXqwTuzrKg...|         23|33-464-151-3439|   -272.6|    BUILDING|ckages. requests ...|\n",
      "|       12|Customer#000000012|       9PWKuhzT4Zr1Q|         13|23-791-276-1263|  3396.49|   HOUSEHOLD| to the carefully...|\n",
      "|       13|Customer#000000013|nsXQu0oVjD7PM659u...|          3|13-761-547-5974|  3857.34|    BUILDING|ounts sleep caref...|\n",
      "|       14|Customer#000000014|     KXkletMlL2JQEA |          1|11-845-129-3851|   5266.3|   FURNITURE|, ironic packages...|\n",
      "|       15|Customer#000000015|YtWggXoOLdwdo7b0y...|         23|33-687-542-7601|  2788.52|   HOUSEHOLD| platelets. regul...|\n",
      "|       16|Customer#000000016| cYiaeMLZSMAOQ2 d0W,|         10|20-781-609-3107|  4681.03|   FURNITURE|kly silent courts...|\n",
      "|       17|Customer#000000017|izrh 6jdqtp2eqdtb...|          2|12-970-682-3487|     6.34|  AUTOMOBILE|packages wake! bl...|\n",
      "|       18|Customer#000000018|3txGO AiuFux3zT0Z...|          6|16-155-215-1315|  5494.43|    BUILDING|s sleep. carefull...|\n",
      "|       19|Customer#000000019|uc,3bHIx84H,wdrmL...|         18|28-396-526-5053|  8914.71|   HOUSEHOLD| nag. furiously c...|\n",
      "|       20|Customer#000000020|       JrPk8Pqplj4Ne|         22|32-957-234-8742|   7603.4|   FURNITURE|g alongside of th...|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f2979c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_df = spark.read.csv(\"C:\\datasets\\datasets\\Retail\\\\nation.csv\",sep=\"\\t\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f7303bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+--------------------+\n",
      "|N_NATIONKEY|    N_NAME|N_REGIONKEY|           N_COMMENT|\n",
      "+-----------+----------+-----------+--------------------+\n",
      "|          0|   ALGERIA|          0| haggle. carefull...|\n",
      "|          1| ARGENTINA|          1|al foxes promise ...|\n",
      "|          2|    BRAZIL|          1|y alongside of th...|\n",
      "|          3|    CANADA|          1|eas hang ironic, ...|\n",
      "|          4|     EGYPT|          4|y above the caref...|\n",
      "|          5|  ETHIOPIA|          0|ven packages wake...|\n",
      "|          6|    FRANCE|          3|refully final req...|\n",
      "|          7|   GERMANY|          3|l platelets. regu...|\n",
      "|          8|     INDIA|          2|ss excuses cajole...|\n",
      "|          9| INDONESIA|          2| slyly express as...|\n",
      "|         10|      IRAN|          4|efully alongside ...|\n",
      "|         11|      IRAQ|          4|nic deposits boos...|\n",
      "|         12|     JAPAN|          2|ously. final, exp...|\n",
      "|         13|    JORDAN|          4|ic deposits are b...|\n",
      "|         14|     KENYA|          0| pending excuses ...|\n",
      "|         15|   MOROCCO|          0|rns. blithely bol...|\n",
      "|         16|MOZAMBIQUE|          0|s. ironic, unusua...|\n",
      "|         17|      PERU|          1|platelets. blithe...|\n",
      "|         18|     CHINA|          2|c dependencies. f...|\n",
      "|         19|   ROMANIA|          3|ular asymptotes a...|\n",
      "+-----------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "665275c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df = spark.read.csv(\"C:\\datasets\\datasets\\Retail\\\\region.csv\",sep=\"\\t\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f18174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+\n",
      "|R_REGIONKEY|     R_NAME|           R_COMMENT|\n",
      "+-----------+-----------+--------------------+\n",
      "|          0|     AFRICA|lar deposits. bli...|\n",
      "|          1|    AMERICA|hs use ironic, ev...|\n",
      "|          2|       ASIA|ges. thinly even ...|\n",
      "|          3|     EUROPE|ly final courts c...|\n",
      "|          4|MIDDLE EAST|uickly special ac...|\n",
      "+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "048f9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nation_regiondf = nation_df.join(region_df,nation_df.N_REGIONKEY == region_df.R_REGIONKEY,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a0f65624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+--------------------+-----------+-----------+--------------------+\n",
      "|N_NATIONKEY|    N_NAME|N_REGIONKEY|           N_COMMENT|R_REGIONKEY|     R_NAME|           R_COMMENT|\n",
      "+-----------+----------+-----------+--------------------+-----------+-----------+--------------------+\n",
      "|          0|   ALGERIA|          0| haggle. carefull...|          0|     AFRICA|lar deposits. bli...|\n",
      "|          1| ARGENTINA|          1|al foxes promise ...|          1|    AMERICA|hs use ironic, ev...|\n",
      "|          2|    BRAZIL|          1|y alongside of th...|          1|    AMERICA|hs use ironic, ev...|\n",
      "|          3|    CANADA|          1|eas hang ironic, ...|          1|    AMERICA|hs use ironic, ev...|\n",
      "|          4|     EGYPT|          4|y above the caref...|          4|MIDDLE EAST|uickly special ac...|\n",
      "|          5|  ETHIOPIA|          0|ven packages wake...|          0|     AFRICA|lar deposits. bli...|\n",
      "|          6|    FRANCE|          3|refully final req...|          3|     EUROPE|ly final courts c...|\n",
      "|          7|   GERMANY|          3|l platelets. regu...|          3|     EUROPE|ly final courts c...|\n",
      "|          8|     INDIA|          2|ss excuses cajole...|          2|       ASIA|ges. thinly even ...|\n",
      "|          9| INDONESIA|          2| slyly express as...|          2|       ASIA|ges. thinly even ...|\n",
      "|         10|      IRAN|          4|efully alongside ...|          4|MIDDLE EAST|uickly special ac...|\n",
      "|         11|      IRAQ|          4|nic deposits boos...|          4|MIDDLE EAST|uickly special ac...|\n",
      "|         12|     JAPAN|          2|ously. final, exp...|          2|       ASIA|ges. thinly even ...|\n",
      "|         13|    JORDAN|          4|ic deposits are b...|          4|MIDDLE EAST|uickly special ac...|\n",
      "|         14|     KENYA|          0| pending excuses ...|          0|     AFRICA|lar deposits. bli...|\n",
      "|         15|   MOROCCO|          0|rns. blithely bol...|          0|     AFRICA|lar deposits. bli...|\n",
      "|         16|MOZAMBIQUE|          0|s. ironic, unusua...|          0|     AFRICA|lar deposits. bli...|\n",
      "|         17|      PERU|          1|platelets. blithe...|          1|    AMERICA|hs use ironic, ev...|\n",
      "|         18|     CHINA|          2|c dependencies. f...|          2|       ASIA|ges. thinly even ...|\n",
      "|         19|   ROMANIA|          3|ular asymptotes a...|          3|     EUROPE|ly final courts c...|\n",
      "+-----------+----------+-----------+--------------------+-----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation_regiondf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5cc919da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_regiondf = nation_regiondf.drop(\"n_comment\",\"r_regionkey\",\"r_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e2f3b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-----------+\n",
      "|N_NATIONKEY|    N_NAME|N_REGIONKEY|     R_NAME|\n",
      "+-----------+----------+-----------+-----------+\n",
      "|          0|   ALGERIA|          0|     AFRICA|\n",
      "|          1| ARGENTINA|          1|    AMERICA|\n",
      "|          2|    BRAZIL|          1|    AMERICA|\n",
      "|          3|    CANADA|          1|    AMERICA|\n",
      "|          4|     EGYPT|          4|MIDDLE EAST|\n",
      "|          5|  ETHIOPIA|          0|     AFRICA|\n",
      "|          6|    FRANCE|          3|     EUROPE|\n",
      "|          7|   GERMANY|          3|     EUROPE|\n",
      "|          8|     INDIA|          2|       ASIA|\n",
      "|          9| INDONESIA|          2|       ASIA|\n",
      "|         10|      IRAN|          4|MIDDLE EAST|\n",
      "|         11|      IRAQ|          4|MIDDLE EAST|\n",
      "|         12|     JAPAN|          2|       ASIA|\n",
      "|         13|    JORDAN|          4|MIDDLE EAST|\n",
      "|         14|     KENYA|          0|     AFRICA|\n",
      "|         15|   MOROCCO|          0|     AFRICA|\n",
      "|         16|MOZAMBIQUE|          0|     AFRICA|\n",
      "|         17|      PERU|          1|    AMERICA|\n",
      "|         18|     CHINA|          2|       ASIA|\n",
      "|         19|   ROMANIA|          3|     EUROPE|\n",
      "+-----------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation_regiondf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c447d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_regnatdf = customer_df.join(nation_regiondf,customer_df.C_NATIONKEY==nation_regiondf.N_NATIONKEY,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b1ba4164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+-----------+-------+-----------+------+\n",
      "|C_CUSTKEY|            C_NAME|           C_ADDRESS|C_NATIONKEY|        C_PHONE|C_ACCTBAL|C_MKTSEGMENT|           C_COMMENT|N_NATIONKEY| N_NAME|N_REGIONKEY|R_NAME|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+-----------+-------+-----------+------+\n",
      "|     1475|Customer#000001475|    4tUf4SaYTFV2H7ji|          0|10-932-794-2009|  1820.28|    BUILDING|uctions sleep bli...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1473|Customer#000001473| UPkONG9dy4VYyGNJGHG|          0|10-891-555-7734|  2796.93|   MACHINERY|uriously. quickly...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1453|Customer#000001453|FTfWkW1 8jVgOIIR9...|          0|10-852-397-3642|   662.67|    BUILDING| to wake above th...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1430|Customer#000001430|   mv 9MEDwd8yPeQj7N|          0|10-209-317-6929|   -920.4|    BUILDING|nic deposits. bol...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1400|Customer#000001400|         BuouRkR7J f|          0|10-217-180-5310|  2432.73|    BUILDING|etect fluffily fi...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1399|Customer#000001399|         FOuY,endAFj|          0|10-775-919-7154|  7352.14|  AUTOMOBILE|foxes across the ...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1283|Customer#000001283|6JeLWEtDERPB,0KzW...|          0|10-203-771-2219|  2222.71|  AUTOMOBILE| blithely daringl...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1247|Customer#000001247|q5,Og3ezW3jSUtbwK...|          0|10-386-173-3167|  1696.95|   MACHINERY|s against the qui...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1226|Customer#000001226|         HKR1zog fXW|          0|10-251-221-9440|  2135.92|   FURNITURE|ns. furiously pen...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1202|Customer#000001202|xThQDiKdG,0sU Idu...|          0|10-788-256-6117|   702.73|    BUILDING|accounts. fluffil...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1197|Customer#000001197|         9A1LTDf0KbR|          0|10-254-891-7835|  9261.05|   FURNITURE|ording to the sly...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1164|Customer#000001164|XWfNRnO2S5KAW0Vod...|          0|10-828-178-5049|  7341.35|   HOUSEHOLD| ideas use. unusu...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1128|Customer#000001128|72XUL0qb4,NLmfyrt...|          0|10-392-200-8982|  8123.99|    BUILDING|odolites accordin...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1122|Customer#000001122|9lxNEW0Rei4DFaT4v...|          0|10-257-957-3327|    45.21|    BUILDING|egular, regular i...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1066|Customer#000001066|2Ge 0Nk29FlBs1GuB...|          0|10-333-463-4472|   949.68|   MACHINERY|requests. slyly f...|          0|ALGERIA|          0|AFRICA|\n",
      "|     1044|Customer#000001044|     Eh2e8gLyStrLE7A|          0|10-451-459-9620|   7291.3|    BUILDING|ly across the sly...|          0|ALGERIA|          0|AFRICA|\n",
      "|      990|Customer#000000990|uF idg4bq8Ij7ghxJ...|          0|10-403-137-1064|  6988.49|    BUILDING|dolites for the f...|          0|ALGERIA|          0|AFRICA|\n",
      "|      973|Customer#000000973|       FT4jTOdVCpmYW|          0|10-749-928-5415|  3229.18|   FURNITURE|sly special reque...|          0|ALGERIA|          0|AFRICA|\n",
      "|      968|Customer#000000968|eu 5FA1WHs9jq0pcd...|          0|10-470-740-2657|  8921.97|    BUILDING|ic foxes haggle s...|          0|ALGERIA|          0|AFRICA|\n",
      "|      955|Customer#000000955|    FIis0dJhR5DwVCLy|          0|10-918-863-8880|   138.31|  AUTOMOBILE|ts cajole quickly...|          0|ALGERIA|          0|AFRICA|\n",
      "+---------+------------------+--------------------+-----------+---------------+---------+------------+--------------------+-----------+-------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_regnatdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e50ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bc26b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmpSchema = StructType([  \n",
    "    StructField('Emp_id', IntegerType(), True), \n",
    "    StructField('Empname', StringType(), True),\n",
    "    StructField('MGR', IntegerType(), True), \n",
    "   StructField('YOJ', StringType(), True), \n",
    "     StructField('dept_id', StringType(), True), \n",
    "    StructField('gender', StringType(), True), \n",
    "     StructField('salary', DoubleType(), True) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d46a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1,\"Smith\",1,\"2018\",\"10\",\"M\",3000.00), \n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000.00), \n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000.00), \n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000.00), \n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",300.00), \n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",2000.00) \n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f927b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDf = spark.createDataFrame(data=emp,schema=EmpSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c686817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|\n",
      "|     6|   Brown|  2|2010|     50|      |2000.0|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d66f0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10), \n",
    "    (\"Marketing\",20), \n",
    "    (\"Sales\",30), \n",
    "    (\"IT\",40) \n",
    "  ]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e24b14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDf = spark.createDataFrame(data=dept,schema=deptColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c0c4aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8678e87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp_id: integer (nullable = true)\n",
      " |-- Empname: string (nullable = true)\n",
      " |-- MGR: integer (nullable = true)\n",
      " |-- YOJ: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ef734697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6dcdd5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#empDf.join(deptDf,\"dept_id\",\"inner\").show()\n",
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "29889837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     6|   Brown|  2|2010|     50|      |2000.0|     NULL|   NULL|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0b67e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname| MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|     4|   Jones|   2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     3|Williams|   1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     1|   Smith|   1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     2|    Rose|   1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|  NULL|    NULL|NULL|NULL|   NULL|  NULL|  NULL|    Sales|     30|\n",
      "|     5|   Brown|   2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa6cc370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname| MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|   1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|   1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     4|   Jones|   2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     2|    Rose|   1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|  NULL|    NULL|NULL|NULL|   NULL|  NULL|  NULL|    Sales|     30|\n",
      "|     5|   Brown|   2|2010|     40|      | 300.0|       IT|     40|\n",
      "|     6|   Brown|   2|2010|     50|      |2000.0|     NULL|   NULL|\n",
      "+------+--------+----+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4b00e7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|\n",
      "+------+--------+---+----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"leftSemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "55117310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+----+-------+------+------+\n",
      "|Emp_id|Empname|MGR| YOJ|dept_id|gender|salary|\n",
      "+------+-------+---+----+-------+------+------+\n",
      "|     6|  Brown|  2|2010|     50|      |2000.0|\n",
      "+------+-------+---+----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0eda35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|    Sales|     30|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf.join(empDf,empDf.dept_id==deptDf.dept_id,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "60f287d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf.join(empDf,empDf.dept_id==deptDf.dept_id,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "35073c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|Emp_id| Empname|MGR| YOJ|dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "|     1|   Smith|  1|2018|     10|     M|3000.0|  Finance|     10|\n",
      "|     3|Williams|  1|2010|     10|     M|1000.0|  Finance|     10|\n",
      "|     4|   Jones|  2|2005|     10|     F|2000.0|  Finance|     10|\n",
      "|     2|    Rose|  1|2010|     20|     M|4000.0|Marketing|     20|\n",
      "|     5|   Brown|  2|2010|     40|      | 300.0|       IT|     40|\n",
      "+------+--------+---+----+-------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,\"cross\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7fe16631",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_dept_df = empDf.join(deptDf,\"dept_id\",\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4dae610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---+----+------+------+---------+\n",
      "|dept_id|Emp_id| Empname|MGR| YOJ|gender|salary|dept_name|\n",
      "+-------+------+--------+---+----+------+------+---------+\n",
      "|     10|     1|   Smith|  1|2018|     M|3000.0|  Finance|\n",
      "|     10|     3|Williams|  1|2010|     M|1000.0|  Finance|\n",
      "|     10|     4|   Jones|  2|2005|     F|2000.0|  Finance|\n",
      "|     20|     2|    Rose|  1|2010|     M|4000.0|Marketing|\n",
      "|     40|     5|   Brown|  2|2010|      | 300.0|       IT|\n",
      "+-------+------+--------+---+----+------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c01dc5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|dept_name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|  Finance|     6000.0|\n",
      "|Marketing|     4000.0|\n",
      "|       IT|      300.0|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df.groupBy(\"dept_name\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "33c0d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|dept_name|totalSal|\n",
      "+---------+--------+\n",
      "|  Finance|  6000.0|\n",
      "|Marketing|  4000.0|\n",
      "|       IT|   300.0|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df.groupBy(\"dept_name\").sum(\"salary\").withColumnRenamed(\"sum(salary)\",\"totalSal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "23f2eb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|dept_name|totalSal|\n",
      "+---------+--------+\n",
      "|  Finance|  6000.0|\n",
      "|Marketing|  4000.0|\n",
      "|       IT|   300.0|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_dept_df.groupBy(\"dept_name\").agg(sum(\"salary\").alias(\"totalSal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "07ed2421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+\n",
      "|dept_name|totalSal|avgSal|\n",
      "+---------+--------+------+\n",
      "|  Finance|  6000.0|2000.0|\n",
      "|Marketing|  4000.0|4000.0|\n",
      "|       IT|   300.0| 300.0|\n",
      "+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df.groupBy(\"dept_name\").agg(sum(\"salary\").alias(\"totalSal\"),avg(\"salary\").alias(\"avgSal\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6e4bb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_saldf = emp_dept_df.groupBy(\"dept_name\",\"gender\").agg(sum(\"salary\").alias(\"totalSal\"),avg(\"salary\").alias(\"avgSal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e362a28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+------+\n",
      "|dept_name|gender|totalSal|avgSal|\n",
      "+---------+------+--------+------+\n",
      "|  Finance|     F|  2000.0|2000.0|\n",
      "|  Finance|     M|  4000.0|2000.0|\n",
      "|       IT|      |   300.0| 300.0|\n",
      "|Marketing|     M|  4000.0|4000.0|\n",
      "+---------+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_saldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c204e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_salnew=dept_saldf.replace(\"\",\"None\",\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dcf75332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+------+\n",
      "|dept_name|gender|totalSal|avgSal|\n",
      "+---------+------+--------+------+\n",
      "|  Finance|     F|  2000.0|2000.0|\n",
      "|  Finance|     M|  4000.0|2000.0|\n",
      "|       IT|  None|   300.0| 300.0|\n",
      "|Marketing|     M|  4000.0|4000.0|\n",
      "+---------+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_salnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1f48cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+------+\n",
      "|dept_name|gender|totalSal|avgSal|\n",
      "+---------+------+--------+------+\n",
      "|  Finance|     F|  2000.0|2000.0|\n",
      "|  Finance|     M|  4000.0|2000.0|\n",
      "|Marketing|     M|  4000.0|4000.0|\n",
      "+---------+------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_salnew=dept_saldf.replace(\"\",None,\"gender\").dropna()\n",
    "dept_salnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8bfc6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDf = spark.read.csv(\"C:\\datasets\\datasets\\employee.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f9821dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|empno|ename| sal|\n",
      "+-----+-----+----+\n",
      "|  111|  zzz|8000|\n",
      "|  111|  aaa|8888|\n",
      "|  121|  bbb|8000|\n",
      "| NULL|  ccc|9000|\n",
      "|false|  ddd|6000|\n",
      "|  555|  eee|7000|\n",
      "|  765| NULL|NULL|\n",
      "|  666|  fff|8890|\n",
      "|  aaa| NULL|NULL|\n",
      "| True| NULL|NULL|\n",
      "| true| NULL|NULL|\n",
      "|    a|    b|   c|\n",
      "|    x| NULL|NULL|\n",
      "| 1000| null|null|\n",
      "|  222| NULL|NULL|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fe156536",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDf = spark.read.option(\"mode\",\"dropmalformed\").csv(\"C:\\datasets\\datasets\\employee.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "26405a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "|empno|ename| sal|\n",
      "+-----+-----+----+\n",
      "|  111|  zzz|8000|\n",
      "|  111|  aaa|8888|\n",
      "|  121|  bbb|8000|\n",
      "| NULL|  ccc|9000|\n",
      "|false|  ddd|6000|\n",
      "|  555|  eee|7000|\n",
      "|  666|  fff|8890|\n",
      "|  aaa| NULL|NULL|\n",
      "|    x| NULL|NULL|\n",
      "| 1000| null|null|\n",
      "|  222| NULL|NULL|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5565c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDf = spark.read.option(\"mode\",\"failfast\").csv(\"C:\\datasets\\datasets\\employee.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f1199275",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1481.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 392.0 failed 1 times, most recent failure: Lost task 0.0 in stage 392.0 (TID 469) (10.33.198.42 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [765,null,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 26 more\r\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1394)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:333)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [765,null,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 26 more\r\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1394)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:333)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[196]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43memployeeDf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\Week6\\Day1\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1481.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 392.0 failed 1 times, most recent failure: Lost task 0.0 in stage 392.0 (TID 469) (10.33.198.42 executor driver): org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [765,null,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 26 more\r\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1394)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:333)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor212.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [765,null,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1611)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:457)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:131)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:366)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:308)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:453)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 26 more\r\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 765\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1394)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:333)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "employeeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7355f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeSchema = StructType([\n",
    "    StructField('Employee_id',IntegerType(),True),\n",
    "    StructField(\"Empname\",StringType(),True),\n",
    "    StructField(\"Salery\",IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "739126eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDf = spark.read.csv(\"C:\\datasets\\datasets\\employee.csv\",header= True,schema = employeeSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "80f9b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_id: integer (nullable = true)\n",
      " |-- Empname: string (nullable = true)\n",
      " |-- Salery: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ec5c7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+\n",
      "|Employee_id|Empname|Salery|\n",
      "+-----------+-------+------+\n",
      "|        111|    zzz|  8000|\n",
      "|        111|    aaa|  8888|\n",
      "|        121|    bbb|  8000|\n",
      "|       NULL|    ccc|  9000|\n",
      "|       NULL|    ddd|  6000|\n",
      "|        555|    eee|  7000|\n",
      "|        765|   NULL|  NULL|\n",
      "|        666|    fff|  8890|\n",
      "|       NULL|   NULL|  NULL|\n",
      "|       NULL|   NULL|  NULL|\n",
      "|       NULL|   NULL|  NULL|\n",
      "|       NULL|      b|  NULL|\n",
      "|       NULL|   NULL|  NULL|\n",
      "|       1000|   null|  NULL|\n",
      "|        222|   NULL|  NULL|\n",
      "+-----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0a53b1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+\n",
      "|Employee_id|Empname|Salery|\n",
      "+-----------+-------+------+\n",
      "|        111|    zzz|  8000|\n",
      "|        111|    aaa|  8888|\n",
      "|        121|    bbb|  8000|\n",
      "|        555|    eee|  7000|\n",
      "|        666|    fff|  8890|\n",
      "+-----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDf = spark.read.option(\"mode\",\"dropmalformed\").csv(\"C:\\datasets\\datasets\\employee.csv\",header= True,schema = employeeSchema)\n",
    "employeeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8e63276a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeeDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ed92dc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5efe9078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1555"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='India'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a0340a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df=sales_df.repartition(8)\n",
    "sales_df.select(\"country\",\"state\",\"city\").filter(\"country='United States'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f18665c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51290"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salesdf_coal_df = sales_df.coalesce(5)\n",
    "salesdf_coal_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4ef3bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange RoundRobinPartitioning(8), REPARTITION_BY_NUM, [plan_id=8223]\n",
      "   +- FileScan csv [ID#3344,OrderID#3345,OrderDate#3346,ShipDate#3347,ShipMode#3348,CustomerID#3349,CustomerName#3350,Segment#3351,City#3352,State#3353,Country#3354,PostalCode#3355,Market#3356,Region#3357,ProductID#3358,Category#3359,Sub-Category#3360,ProductName#3361,Sales#3362,Quantity#3363,Discount#3364,Profit#3365,ShippingCost#3366,OrderPriority#3367] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/datasets/datasets/superstore.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,OrderID:string,OrderDate:string,ShipDate:string,ShipMode:string,CustomerID:string,C...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63c92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad44206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|      country|          state|\n",
      "+-------------+---------------+\n",
      "|United States|       New York|\n",
      "|    Australia|New South Wales|\n",
      "|    Australia|     Queensland|\n",
      "|      Germany|         Berlin|\n",
      "|      Senegal|          Dakar|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select country,state from sales limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae2091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
